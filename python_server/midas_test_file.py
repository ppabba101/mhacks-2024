# -*- coding: utf-8 -*-
"""Copy of intelisl_midas_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e169k0WJaADrKmxWN-QvCtTWohrpoWbK

### This notebook is optionally accelerated with a GPU runtime.
### If you would like to use this acceleration, please select the menu option "Runtime" -> "Change runtime type", select "Hardware Accelerator" -> "GPU" and click "SAVE"

----------------------------------------------------------------------

# MiDaS

*Author: Intel ISL*

**MiDaS models for computing relative depth from a single image.**

<img src="https://pytorch.org/assets/images/midas_samples.png" alt="alt" width="50%"/>


### Model Description

[MiDaS](https://arxiv.org/abs/1907.01341) computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using
multi-objective optimization to ensure high quality on a wide range of inputs.

### Dependencies

MiDaS depends on [timm](https://github.com/rwightman/pytorch-image-models). Install with
"""


"""### Example Usage

Download an image from the PyTorch homepage
"""

import cv2
import torch
import urllib.request
# from MiDaS.midas.dpt_depth import DPTDepthModel

# from midas.dpt_depth import DPTDepthModel

import matplotlib.pyplot as plt

# url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
# urllib.request.urlretrieve(url, filename)
filename = "IMG_2134.jpeg"

print("Filename set to:", filename)

"""Load a model (see [https://github.com/intel-isl/MiDaS/#Accuracy](https://github.com/intel-isl/MiDaS/#Accuracy) for an overview)"""

# model_type = "DPT_Large"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)
# model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)
model_type = "MiDaS_small"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)

print("Loading model:", model_type)
midas = torch.hub.load("intel-isl/MiDaS", model_type)
print("Model loaded successfully")

"""Move model to GPU if available"""

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print("Using device:", device)
midas.to(device)
midas.eval()
print("Model moved to device and set to evaluation mode")

"""Load transforms to resize and normalize the image for large or small model"""

print("Loading transforms")
midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")

if model_type == "DPT_Large" or model_type == "DPT_Hybrid":
    transform = midas_transforms.dpt_transform
else:
    transform = midas_transforms.small_transform
print("Transforms loaded successfully")

"""Load image and apply transforms"""

print("Loading image:", filename)
img = cv2.imread(filename)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
print("Image loaded and converted to RGB")

input_batch = transform(img).to(device)
print("Transforms applied to image and moved to device")

"""Predict and resize to original resolution"""

import time

print("Starting prediction")
with torch.no_grad():
    start_time = time.time()
    prediction = midas(input_batch)
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"Execution time: {execution_time:.4f} seconds")

    prediction = torch.nn.functional.interpolate(
        prediction.unsqueeze(1),
        size=img.shape[:2],
        mode="bicubic",
        align_corners=False,
    ).squeeze()
    print("Prediction resized to original resolution")

output = prediction.cpu().numpy()
print("Prediction moved to CPU and converted to numpy array")

"""Show result"""

plt.imshow(output, cmap='gray')
plt.show()
print("Result displayed")

"""### References
[Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer](https://arxiv.org/abs/1907.01341)

[Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)

Please cite our papers if you use our models:
"""